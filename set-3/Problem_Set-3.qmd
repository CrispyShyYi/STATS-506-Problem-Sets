---
title: "Problem Set 3"
author: "Jialiang Wu"
format:
  html:
    theme: default
    page-layout: article
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
editor: visual
---

For source files and data, see my GitHub repo: [STATS-506-Problem-Sets-03](https://github.com/CrispyShyYi/STATS-506-Problem-Sets/tree/main/set-3)

## Set up

```{r}
library(knitr)
library(dplyr)
library(haven) # xpt data set
library(pscl)
library(RSQLite)
library(DBI) 
library(microbenchmark)
library(stringr)
library(ggplot2)
```

## Question 1

### question 1-a

load two data sets and inner merge.

```{r q1a}
aux <- haven::read_xpt("AUX_I.xpt")
demo <- haven::read_xpt("DEMO_I.xpt")

# check the SEQN is unique 
stopifnot(n_distinct(demo$SEQN) == nrow(demo))
stopifnot(n_distinct(aux$SEQN)  == nrow(aux))

# merge
merged <- demo %>% inner_join(aux, by = "SEQN")

cat("num of rows of DEMO_I:", nrow(demo), "\n")
cat("num of rows of AUX_I:", nrow(aux),  "\n")
cat("num of merged `data.frame`:", nrow(merged), "\n")
```

### question 1-b

In this case, there is a special column: INDHHIN2 - Annual household income. I decided to seperate into two groups: `Under $20,000` and `$20,000 and Over`, which could include all the data.

```{r q1b}
# convert to category according to the doc
cleaned <- merged %>%
  transmute(
    # targets
    Tympanometric_width_of_right_ear = AUXTWIDR,
    Tympanometric_width_of_left_ear = AUXTWIDL,
    
    # predictors
    # Gender (1=Male, 2=Female)
    Gender = factor(RIAGENDR,
                    levels = c(1, 2),
                    labels = c("Male", "Female")),
    
    # Citizenship (1=Citizen by birth or naturalization, 2=Not a citizen of the US)
    Citizenship = na_if(DMDCITZN, 7),   # 7 = Refused
    Citizenship = na_if(Citizenship, 9), # 9 = Don't know
    Citizenship = factor(Citizenship,
                         levels = c(1, 2),
                         labels = c("US Citizen (birth/naturalization)",
                                    "Not a citizen of the US")),
    
    # Number of children 5 years or younger in the household ('0'=0, '1'=1, '2'=2, '3'=3 or more)
    Num5Children = factor(DMDHHSZA,
                         levels = c(0, 1, 2, 3),
                         labels = c("0", "1", "2", "3 or more")),
    
    # Annual household income (INDHHIN2)
    AnnualHouseIncome = case_when(
      INDHHIN2 %in% c(1, 2, 3, 4, 13) ~ "Under $20,000",
      INDHHIN2 %in% c(5, 6, 7, 8, 9, 10, 14, 15, 12) ~ "$20,000 and Over",
      INDHHIN2 %in% c(77, 99) ~ NA_character_,  # Refused / Don't know -> NA
      TRUE ~ NA_character_                      # missing -> NA
    ),
    AnnualHouseIncome = factor(AnnualHouseIncome, 
                          levels = c("Under $20,000", "$20,000 and Over"))
  )

# check cleaned dataframe
str(cleaned)
```

### question 1-c

```{r q1c}
# transfer Num5Children and AnnualHouseIncome to continuous
cleaned <- cleaned %>%
  mutate(
    # Num5Children to int
    Num5Children_n = case_when(
      Num5Children == "0" ~ 0,
      Num5Children == "1" ~ 1,
      Num5Children == "2" ~ 2,
      Num5Children == "3 or more" ~ 3,
      TRUE ~ NA_real_
    ),
    # AnnualHouseIncome to int
    AnnualHouseIncome_n = case_when(
      AnnualHouseIncome == "Under $20,000" ~ 0,
      AnnualHouseIncome == "$20,000 and Over" ~ 1,
      TRUE ~ NA_real_
    )
  )

# Poisson regression model 1 - Right ear: gender
R1 <- glm(
  Tympanometric_width_of_right_ear ~ Gender,
  family = poisson(link = "log"),
  data = cleaned
)
summary(R1)

# Poisson regression model 2 - Right ear: gender, citizenship status (as categorical), number of children (as continuous), annual household income (as continuous)
R2 <- glm(
  Tympanometric_width_of_right_ear ~ Gender + Citizenship + Num5Children_n + AnnualHouseIncome_n,
  family = poisson(link = "log"),
  data = cleaned
)
summary(R2)

# Poisson regression model 3 - Left ear: gender
L1 <- glm(
  Tympanometric_width_of_left_ear ~ Gender,
  family = poisson(link = "log"),
  data = cleaned
)
summary(L1)

# Poisson regression model 4 - Left ear: gender, citizenship status (as categorical), number of children (as continuous), annual household income (as continuous)
L2 <- glm(
  Tympanometric_width_of_left_ear ~ Gender + Citizenship + Num5Children_n + AnnualHouseIncome_n,
  family = poisson(link = "log"),
  data = cleaned
)
summary(L2)

# help function - get incidence risk ratios, N, AIC and pseudo-R^2
get_model_info <- function(model) {
  irr <- exp(coef(model))
  ci <- exp(confint(model))
  tibble(
    term = names(irr),
    IRR = irr,
    CI_low = ci[,1],
    CI_high = ci[,2]
  ) %>%
    mutate(
      N = nobs(model),
      AIC = AIC(model),
      PseudoR2 = pscl::pR2(model)["McFadden"]
    )
}

# get information
R1_res <- get_model_info(R1)
R2_res <- get_model_info(R2)
L1_res <- get_model_info(L1)
L2_res <- get_model_info(L2)

# combine information
all_results <- bind_rows(
  R1_res %>% mutate(Model = "Right ear - Gender"),
  R2_res %>% mutate(Model = "Right ear - Full"),
  L1_res %>% mutate(Model = "Left ear - Gender"),
  L2_res %>% mutate(Model = "Left ear - Full")
)

kable(all_results, digits = 3, caption = "Poisson regression results: Incidence Rate Ratios (IRR)")
```

### question 1-d

```{r q1d}
# IRR
print("Coefficient of the L2 Model")
exp(coef(L2)) 

# confidence interval
print("Confidence Interval of the coefficients")
exp(confint(L2)) 

# summary table
summary(L2)
```

In Model 2L, gender has a significant effect on the Tympanometric width of the left ear. For females compared to males, the estimated log(IRR) is 0.015 (SE = 0.0035, z = 4.41, p \< 0.001). Transformed to the incidence risk ratio scale, this corresponds to an IRR of 1.016, meaning that females have, on average, about a 1.6% higher expected Tympanometric width than males, after controlling for citizenship status, number of children, and annual household income.

## Question 2

connect db

```{r connect}
db_path <- "sakila_master.db"
sakila <- dbConnect(RSQLite::SQLite(), db_path)

# List tables 
dbListTables(sakila)
```

### question 2-a

```{r q2a}
# dive into customer table
dbListFields(sakila, "customer")

# use SQL get table, then use R
customer_df <- dbGetQuery(sakila, "SELECT customer_id, store_id, active FROM customer")
# check types
str(customer_df)

approach_r_1 <- function() {
  customer_df <- dbGetQuery(
    sakila,
    "SELECT customer_id, store_id, active FROM customer"
  )
  customer_df %>%
    mutate(active_num = as.integer(active)) %>%   # "1"/"0" -> 1/0
    group_by(store_id) %>%
    summarise(
      total_customers  = n(),
      active_customers = sum(active_num, na.rm = TRUE),
      pct_active       = round(100 * mean(active_num, na.rm = TRUE), 2),
      .groups = "drop"
    ) %>%
    arrange(store_id)
}

# use SQL
approach_sql_1 <- function() {
  dbGetQuery(
    sakila,
    "
    SELECT
      store_id,
      COUNT(customer_id) AS total_customers,
      SUM(active) AS active_customers,
      ROUND(100.0 * SUM(active ) / COUNT(customer_id), 2) AS pct_active
    FROM customer
    GROUP BY store_id
    "
  )
}

# display results
res_r_1   <- approach_r_1()
res_sql_1 <- approach_sql_1()

print(res_r_1)
print(res_sql_1)

# microbenchmark comparison
microbenchmark(
  R_aggregate   = approach_r_1(),
  SQL_aggregate = approach_sql_1()
)
```

According to the benchmark comparision, the SQL run faster than the SQL + R strategy

### question 2-b

```{r q2b}
# dive into corresponding tables
dbListFields(sakila, "staff")
dbListFields(sakila, "address")
dbListFields(sakila, "city")
dbListFields(sakila, "country")

# SQL + R
approach_r_2 <- function() {
  staff_df   <- dbGetQuery(sakila, "SELECT staff_id, first_name, last_name, address_id FROM staff")
  address_df <- dbGetQuery(sakila, "SELECT address_id, city_id FROM address")
  city_df    <- dbGetQuery(sakila, "SELECT city_id, country_id FROM city")
  country_df <- dbGetQuery(sakila, "SELECT country_id, country FROM country")
  
  staff_country_r <- staff_df %>%
  inner_join(address_df, by = "address_id") %>%
  inner_join(city_df, by = "city_id") %>%
  inner_join(country_df, by = "country_id") %>%
  transmute(staff_name = paste(first_name, last_name),
            country)
}

# SQL
approach_sql_2 <- function() {
  dbGetQuery(
    sakila,
    "
    SELECT s.first_name || ' ' || s.last_name AS staff_name,
       ctry.country AS country
    FROM staff s
    INNER JOIN address a ON s.address_id = a.address_id
    INNER JOIN city c ON a.city_id = c.city_id
    INNER JOIN country ctry ON c.country_id = ctry.country_id
    "
  )
}

# display results
res_r_2   <- approach_r_2()
res_sql_2 <- approach_sql_2()

print(res_r_2)
print(res_sql_2)

# benchmark comparison
microbenchmark(
  R_aggregate   = approach_r_2(),
  SQL_aggregate = approach_sql_2()
)
```

According to the benchmark comparasion, the SQL run faster than the SQL + R strategy

### question 2-c

```{r q2c}
# dive into corresponding tables
dbListFields(sakila, "film")
dbListFields(sakila, "inventory")
dbListFields(sakila, "rental")
dbListFields(sakila, "payment")

# SQL + R
approach_r_3 <- function() {
  film_df      <- dbGetQuery(sakila, "SELECT film_id, title FROM film")
  inventory_df <- dbGetQuery(sakila, "SELECT inventory_id, film_id FROM inventory")
  rental_df    <- dbGetQuery(sakila, "SELECT rental_id, inventory_id FROM rental")
  payment_df   <- dbGetQuery(sakila, "SELECT rental_id, amount FROM payment")
  
  film_rev <- film_df %>%
    inner_join(inventory_df, by = "film_id") %>%
    inner_join(rental_df,    by = "inventory_id") %>%
    inner_join(payment_df,   by = "rental_id") %>%
    group_by(film_id, title) %>%
    summarise(film_revenue = sum(amount, na.rm = TRUE), .groups = "drop")
  
  max_rev <- max(film_rev$film_revenue, na.rm = TRUE)
  
  top_films <- film_rev %>%
    filter(film_revenue == max_rev) %>%
    arrange(title)
  
  top_films
}

# SQL
approach_sql_3 <- function() {
  dbGetQuery(
    sakila,
    "
    SELECT f.title,
       SUM(p.amount) AS film_revenue
    FROM film f
    JOIN inventory i ON i.film_id = f.film_id
    JOIN rental    r ON r.inventory_id = i.inventory_id
    JOIN payment   p ON p.rental_id    = r.rental_id
    GROUP BY f.film_id, f.title
    HAVING SUM(p.amount) = (
      SELECT MAX(film_revenue)
      FROM (
        SELECT SUM(p2.amount) AS film_revenue
        FROM film f2
        JOIN inventory i2 ON i2.film_id = f2.film_id
        JOIN rental    r2 ON r2.inventory_id = i2.inventory_id
        JOIN payment   p2 ON p2.rental_id    = r2.rental_id
        GROUP BY f2.film_id
      )
    )
    ORDER BY f.title
    "
  )
}

# display results
res_r_3   <- approach_r_3()
res_sql_3 <- approach_sql_3()

print(res_r_3)
print(res_sql_3)

# benchmark comparison
microbenchmark(
  R_aggregate   = approach_r_3(),
  SQL_aggregate = approach_sql_3()
)
```

According to the benchmark comparasion, the SQL + R strategy run faster than the SQL

## Question 3

set up data frame

```{r Australia-500Records}
# load data set
Australia <- read.csv("au-500.csv")
```

### question 3-a

```{r q3a}
# count web ends with ".com"
pattern <- "\\.com\\s*$"

is_com <- grepl(pattern, Australia$web, ignore.case = TRUE)
pct_com <- mean(is_com, na.rm = TRUE) * 100
cat("There is", pct_com, "% of websites are .com’s")
```

### question 3-b

```{r q3b}
Australia %>%
  count(domain = sub(".*@", "", email)) %>%
  arrange(desc(n)) %>%
  slice(1)
```

According to the result, the hotmail.com is the most used domain

### question 3-c

```{r q3c}
# contain a non-alphabetic character, excluding commas and whitespace
prop_non_alpha <- mean(str_detect(Australia$company_name, "[^A-Za-z,\\s]"))
cat("There are", prop_non_alpha * 100, "% company names contain a non-alphabetic character, excluding commas and whitespace\n")

# contain a non-alphabetic character, excluding commas, whitespace, and ampersands (“&”)
prop_non_alpha_no_amp <- mean(str_detect(Australia$company_name, "[^A-Za-z,\\s&]"))
cat("There are", prop_non_alpha_no_amp * 100, "% company names contain a non-alphabetic character, excluding commas, whitespace, and ampersands (“&”)\n")
```

### question 3-d

```{r q3d}
Australia <- Australia %>%
  mutate(
    phone1 = gsub("-", "", phone1),  # get rid of '-'
    phone1 = sub("^(\\d{4})(\\d{3})(\\d{3})$", "\\1-\\2-\\3", phone1),
    phone2 = gsub("-", "", phone2),  # get rid of '-'
    phone2 = sub("^(\\d{4})(\\d{3})(\\d{3})$", "\\1-\\2-\\3", phone2),
  )

# check
head(Australia[, c("phone1", "phone2")], 10)
```

### question 3-e

```{r q3e}
Australia <- Australia %>%
  mutate(
    # extract apartment number after #
    # if there is no apartment number, keep as NA
    apartment_number = as.numeric(sub(".*#(\\d+)$", "\\1", address))
  )

# plot
Australia %>%
  filter(!is.na(apartment_number)) %>%
  ggplot(aes(x = log(apartment_number))) +
  geom_histogram(binwidth = 0.5, fill = "steelblue", color = "black") +
  labs(
    title = "Histogram of log apartment numbers",
    x = "log(apartment number)",
    y = "Count"
  )
```

### question 3-f

```{r q3f}
# extract the first digit
benford_data <- Australia %>%
  filter(!is.na(apartment_number)) %>%
  mutate(leading_digit = as.numeric(substr(apartment_number, 1, 1))) %>%
  count(leading_digit) %>%
  mutate(freq = n / sum(n))

# theorem distribution
benford_theory <- data.frame(
  leading_digit = 1:9,
  prob = log10(1 + 1 / (1:9))
)

# visualization
ggplot() +
  geom_bar(data = benford_data, aes(x = factor(leading_digit), y = freq), 
           stat = "identity", fill = "steelblue", alpha = 0.7) +
  geom_line(data = benford_theory, aes(x = factor(leading_digit), y = prob, group = 1), 
            color = "red") +
  geom_point(data = benford_theory, aes(x = factor(leading_digit), y = prob), 
             color = "red") +
  labs(
    title = "Apartment Numbers vs. Benford's Law",
    x = "Leading Digit",
    y = "Frequency"
  )
```

According to the visualization plot, the apartment number in this Austrilia - 500 data set doesn't follow the Benford’s law. Instead, the leading digits are approximately uniform distributed. So I think the apartment number would not pass as real data.
